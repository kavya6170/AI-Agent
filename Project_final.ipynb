{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavya6170/AI-Agent/blob/main/Project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd1Ka1WjoqJq"
      },
      "outputs": [],
      "source": [
        "!pip install -U \\\n",
        "langchain \\\n",
        "langchain-community \\\n",
        "langchain-core \\\n",
        "langchain-google-genai \\\n",
        "google-generativeai \\\n",
        "sentence-transformers \\\n",
        "faiss-cpu \\\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "W31kEIVFox4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ======================================================\n",
        "# GEMINI API KEY\n",
        "# ======================================================\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBTOQynnT2ZivauDAvp6TkM1FTOSv9QEWE\"\n",
        "\n",
        "# ======================================================\n",
        "# PATHS & STORAGE\n",
        "# ======================================================\n",
        "DATA_DIR = \"/content/drive/MyDrive/Project\" # data path\n",
        "VECTOR_DIR = \"/content/drive/MyDrive/vector_store\"# this is the path where embeddings will be saved\n",
        "\n",
        "DOC_INDEX_PATH = os.path.join(VECTOR_DIR, \"doc_index.faiss\")# to store the index of vectors\n",
        "TEXTS_PATH = os.path.join(VECTOR_DIR, \"texts.pkl\")# chunk texts\n",
        "METADATA_PATH = os.path.join(VECTOR_DIR, \"metadata.pkl\")# chunk metadata\n",
        "INDEXED_FILES_PATH = os.path.join(VECTOR_DIR, \"indexed_files.json\")#list of indexed PDFs\n",
        "\n",
        "RF_MODEL_PATH = \"/content/drive/MyDrive/hallucination_RF_model.pkl\"\n",
        "\n",
        "os.makedirs(VECTOR_DIR, exist_ok=True) # if the vector is not existing then it will create\n",
        "\n",
        "# ======================================================\n",
        "# CONFIGURATION\n",
        "# ======================================================\n",
        "CHUNK_SIZE = 600 # chunk size\n",
        "CHUNK_OVERLAP = 120 # chunk overlap\n",
        "\n",
        "TOP_K = 3 #Retrieve top-3 most similar chunks\n",
        "MAX_CONTEXT_CHARS = 1600 #Prevents exceeding LLM context limits\n",
        "SIMILARITY_THRESHOLD = 2.2 #FAISS L2 distance threshold\n",
        "MEMORY_SIMILARITY_THRESHOLD = 1.2 #Controls memory reuse sensitivity\n",
        "MAX_SUBQUERIES = 6 #Upper limit for query decomposition\n",
        "\n",
        "DISALLOWED_KEYWORDS = [\n",
        "    \"rag\", \"llm\", \"ai\", \"machine learning\",\n",
        "    \"deep learning\", \"transformer\"\n",
        "]\n",
        "\n",
        "# Embedding model (MUST match training)\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "dimension = embedder.get_sentence_embedding_dimension()\n",
        "\n",
        "# Load trained Random Forest model\n",
        "rf_model = joblib.load(RF_MODEL_PATH)\n",
        "print(\"âœ… Random Forest hallucination model loaded\")\n",
        "\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-latest\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# ======================================================\n",
        "# LOAD INDEXED FILES\n",
        "# ======================================================\n",
        "if os.path.exists(INDEXED_FILES_PATH):\n",
        "    with open(INDEXED_FILES_PATH, \"r\") as f:\n",
        "        indexed_files = set(json.load(f))\n",
        "else:\n",
        "    indexed_files = set()\n",
        "\n",
        "# ======================================================\n",
        "# LOAD OR CREATE FAISS INDEX\n",
        "# ======================================================\n",
        "if os.path.exists(DOC_INDEX_PATH):\n",
        "    print(\"ðŸ”„ Loading existing FAISS index...\")\n",
        "    doc_index = faiss.read_index(DOC_INDEX_PATH)\n",
        "\n",
        "    with open(TEXTS_PATH, \"rb\") as f:\n",
        "        texts = pickle.load(f)\n",
        "\n",
        "    with open(METADATA_PATH, \"rb\") as f:\n",
        "        metadata = pickle.load(f)\n",
        "else:\n",
        "    print(\"ðŸ†• Creating new FAISS index...\")\n",
        "    doc_index = faiss.IndexFlatL2(dimension)\n",
        "    texts = []\n",
        "    metadata = []\n",
        "\n",
        "# ======================================================\n",
        "# INCREMENTAL DOCUMENT INGESTION\n",
        "# ======================================================\n",
        "new_documents = []\n",
        "\n",
        "for file in os.listdir(DATA_DIR):\n",
        "    if file.endswith(\".pdf\") and file not in indexed_files:\n",
        "        loader = PyPDFLoader(os.path.join(DATA_DIR, file))\n",
        "        pages = loader.load()\n",
        "        for p in pages:\n",
        "            p.metadata[\"source\"] = file\n",
        "        new_documents.extend(pages)\n",
        "        indexed_files.add(file)\n",
        "\n",
        "if new_documents:\n",
        "    print(f\"ðŸ†• Found {len(new_documents)} new pages\")\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "\n",
        "    new_chunks = splitter.split_documents(new_documents)\n",
        "    new_texts = [c.page_content for c in new_chunks]\n",
        "    new_metadata = [c.metadata for c in new_chunks]\n",
        "\n",
        "    new_embeddings = embedder.encode(new_texts, show_progress_bar=True)\n",
        "\n",
        "    doc_index.add(np.array(new_embeddings))\n",
        "    texts.extend(new_texts)\n",
        "    metadata.extend(new_metadata)\n",
        "\n",
        "    faiss.write_index(doc_index, DOC_INDEX_PATH)\n",
        "\n",
        "    with open(TEXTS_PATH, \"wb\") as f:\n",
        "        pickle.dump(texts, f)\n",
        "\n",
        "    with open(METADATA_PATH, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    with open(INDEXED_FILES_PATH, \"w\") as f:\n",
        "        json.dump(list(indexed_files), f)\n",
        "\n",
        "    print(\"âœ… New embeddings added & saved\")\n",
        "else:\n",
        "    print(\"âœ… No new documents found\")\n",
        "\n",
        "# ======================================================\n",
        "# MEMORY VECTOR STORE\n",
        "# ======================================================\n",
        "memory_texts = []\n",
        "memory_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "def search_memory(question):\n",
        "    if not memory_texts:\n",
        "        return None\n",
        "    q_emb = embedder.encode([question])\n",
        "    dist, idx = memory_index.search(np.array(q_emb), 1)\n",
        "    if dist[0][0] < MEMORY_SIMILARITY_THRESHOLD:\n",
        "        return memory_texts[idx[0][0]]\n",
        "    return None\n",
        "\n",
        "def store_memory(question, answer):\n",
        "    entry = f\"Q: {question}\\nA: {answer}\"\n",
        "    emb = embedder.encode([entry])\n",
        "    memory_index.add(np.array(emb))\n",
        "    memory_texts.append(entry)\n",
        "\n",
        "def retrieve_chunks(query):\n",
        "    q_emb = embedder.encode([query])\n",
        "    distances, indices = doc_index.search(np.array(q_emb), TOP_K)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if dist < SIMILARITY_THRESHOLD:\n",
        "            results.append({\n",
        "                \"text\": texts[idx],\n",
        "                \"source\": metadata[idx][\"source\"]\n",
        "            })\n",
        "    return results\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# PROMPTS\n",
        "# ======================================================\n",
        "subquery_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Break the question into 3â€“6 focused sub-questions.\n",
        "Do NOT answer.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return only a numbered list.\n",
        "\"\"\")\n",
        "\n",
        "def split_into_subqueries(question):\n",
        "    resp = llm.invoke(subquery_prompt.format_messages(question=question))\n",
        "    lines = resp.content.split(\"\\n\")\n",
        "\n",
        "    subs = []\n",
        "    for l in lines:\n",
        "        if l.strip() and l[0].isdigit():\n",
        "            subs.append(l.split(\".\", 1)[1].strip())\n",
        "\n",
        "    return subs[:MAX_SUBQUERIES]\n",
        "\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a document-grounded enterprise policy assistant.\n",
        "\n",
        "Rules:\n",
        "- Use ONLY provided context\n",
        "- No external knowledge\n",
        "- Elaborate answer in 4-5 lines\n",
        "- give it in bullet points if needed\n",
        "- If missing, say exactly:\n",
        "  \"Information not found in the provided documents.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "#RANDOM FOREST FEATURE EXTRACTION\n",
        "def token_overlap_ratio(answer, context):\n",
        "    a = set(answer.lower().split())\n",
        "    c = set(context.lower().split())\n",
        "    if not a:\n",
        "        return 0.0\n",
        "    return len(a & c) / len(a)\n",
        "\n",
        "def extract_rf_features(question, context, answer):\n",
        "    q_emb = embedder.encode(question)\n",
        "    c_emb = embedder.encode(context)\n",
        "    a_emb = embedder.encode(answer if answer else question)\n",
        "\n",
        "    return np.array([\n",
        "        cosine_similarity([q_emb], [c_emb])[0][0],\n",
        "        cosine_similarity([a_emb], [c_emb])[0][0],\n",
        "        cosine_similarity([q_emb], [a_emb])[0][0],\n",
        "        token_overlap_ratio(answer, context),\n",
        "        len(context.split()),\n",
        "        len(answer.split())\n",
        "    ]).reshape(1, -1)\n",
        "\n",
        "\n",
        "def extract_text_from_response(response):\n",
        "    content = response.content\n",
        "\n",
        "    # Case 1: content is already a string\n",
        "    if isinstance(content, str):\n",
        "        return content.strip()\n",
        "\n",
        "    # Case 2: Gemini returns list of parts\n",
        "    if isinstance(content, list):\n",
        "        texts = []\n",
        "        for part in content:\n",
        "            if isinstance(part, dict) and \"text\" in part:\n",
        "                texts.append(part[\"text\"])\n",
        "            elif hasattr(part, \"text\"):\n",
        "                texts.append(part.text)\n",
        "        return \" \".join(texts).strip()\n",
        "\n",
        "    # Fallback\n",
        "    return str(content).strip()\n",
        "\n",
        "def is_definition_question(question):\n",
        "    q = question.lower().strip()\n",
        "    return q.startswith(\"what is\") or q.startswith(\"define\")\n",
        "\n",
        "\n",
        "def is_procedure_question(question):\n",
        "    procedure_keywords = [\n",
        "        \"how to\", \"how do i\", \"process\", \"procedure\",\n",
        "        \"steps\", \"file\", \"complaint\", \"apply\"\n",
        "    ]\n",
        "    q = question.lower()\n",
        "    return any(k in q for k in procedure_keywords)\n",
        "\n",
        "\n",
        "def is_scenario_question(question):\n",
        "    scenario_starters = [\n",
        "        \"can i\", \"is it allowed\", \"is it okay\",\n",
        "        \"what if\", \"suppose\", \"would it be\",\n",
        "        \"am i allowed\"\n",
        "    ]\n",
        "    q = question.lower()\n",
        "    return any(q.startswith(s) for s in scenario_starters)\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# MAIN RAG FUNCTION\n",
        "# ======================================================\n",
        "def rag_answer(question):\n",
        "\n",
        "    # ===============================\n",
        "    # 1ï¸âƒ£ MEMORY LOOKUP (skip scenarios)\n",
        "    # ===============================\n",
        "    def is_cross_policy_question(question):\n",
        "      keywords = [\n",
        "        \"interact\", \"between\", \"relation\", \"difference\",\n",
        "        \"versus\", \"vs\", \"compare\", \"different\",\n",
        "        \"exempt\", \"apply to\", \"applicable to\"\n",
        "      ]\n",
        "      q = question.lower()\n",
        "      return any(k in q for k in keywords)\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 1ï¸âƒ£ MEMORY LOOKUP (SAFE CASES)\n",
        "    # ===============================\n",
        "    if (\n",
        "    not is_scenario_question(question)\n",
        "    and not is_cross_policy_question(question)\n",
        "    ):\n",
        "      memory_hit = search_memory(question)\n",
        "      if memory_hit:\n",
        "        return memory_hit, {\"memory\"}, {\n",
        "            \"hallucinated\": False,\n",
        "            \"confidence\": None,\n",
        "            \"reason\": \"memory_reuse\"\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 2ï¸âƒ£ DISALLOWED KEYWORDS\n",
        "    # ===============================\n",
        "    if any(w in question.lower() for w in DISALLOWED_KEYWORDS):\n",
        "      return \"Information not found in the provided documents.\", set(), {\n",
        "    \"hallucinated\": True,\n",
        "    \"confidence\": None,\n",
        "    \"reason\": \"unanswerable\"\n",
        "    }\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 3ï¸âƒ£ RETRIEVAL STRATEGY (INTENT-AWARE)\n",
        "    # ===============================\n",
        "    retrieved = []\n",
        "\n",
        "    if is_definition_question(question):\n",
        "        retrieved = retrieve_chunks(question)\n",
        "\n",
        "    elif is_scenario_question(question):\n",
        "        retrieved = retrieve_chunks(question)\n",
        "\n",
        "    elif is_procedure_question(question):\n",
        "        for sq in split_into_subqueries(question):\n",
        "            retrieved.extend(retrieve_chunks(sq))\n",
        "\n",
        "    else:\n",
        "        retrieved = retrieve_chunks(question)\n",
        "\n",
        "    if not retrieved:\n",
        "        return \"Information not found in the provided documents.\", set(), {\n",
        "    \"hallucinated\": True,\n",
        "    \"confidence\": None,\n",
        "    \"reason\": \"unanswerable\"\n",
        "    }\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 4ï¸âƒ£ CONTEXT BUILDING\n",
        "    # ===============================\n",
        "    context = \"\"\n",
        "    sources = set()\n",
        "\n",
        "    for r in retrieved:\n",
        "        if len(context) + len(r[\"text\"]) > MAX_CONTEXT_CHARS:\n",
        "            break\n",
        "        context += r[\"text\"] + \"\\n\\n\"\n",
        "        sources.add(r[\"source\"])\n",
        "\n",
        "    # Scenario-specific caution\n",
        "    if is_scenario_question(question):\n",
        "        context = (\n",
        "            \"Use the following policy rules, restrictions, \"\n",
        "            \"and conditions to answer cautiously.\\n\\n\"\n",
        "            + context\n",
        "        )\n",
        "\n",
        "    # ===============================\n",
        "    # 5ï¸âƒ£ LLM CALL (ChatPromptTemplate)\n",
        "    # ===============================\n",
        "    response = llm.invoke(\n",
        "        answer_prompt.format_messages(\n",
        "            context=context,\n",
        "            question=question\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Gemini-safe extraction\n",
        "    answer = extract_text_from_response(response)\n",
        "\n",
        "    # ===============================\n",
        "    # 6ï¸âƒ£ RANDOM FOREST HALLUCINATION CHECK\n",
        "    # ===============================\n",
        "    X = extract_rf_features(\n",
        "        question=question,\n",
        "        context=context,\n",
        "        answer=answer\n",
        "    )\n",
        "\n",
        "    rf_prob = rf_model.predict_proba(X)[0][1]\n",
        "\n",
        "    # stricter threshold for scenario questions\n",
        "    threshold = 0.75 if is_scenario_question(question) else 0.6\n",
        "    MIN_CONFIDENCE = 0.6\n",
        "\n",
        "    rule_flag = False\n",
        "\n",
        "    if \"matrix\" in answer.lower() and \"matrix\" not in context.lower():\n",
        "      rule_flag = True\n",
        "\n",
        "    if \"http\" in answer.lower() and \"http\" not in context.lower():\n",
        "      rule_flag = True\n",
        "\n",
        "    UNANSWERABLE_TEXT = \"information not found in the provided documents.\"\n",
        "\n",
        "    # ===============================\n",
        "    # FINAL DECISION LOGIC (3 STATES)\n",
        "    # ===============================\n",
        "\n",
        "    if answer.strip().lower() == UNANSWERABLE_TEXT:\n",
        "      rf_result = {\n",
        "        \"hallucinated\": True,\n",
        "        \"confidence\": round(rf_prob, 3),\n",
        "        \"reason\": \"unanswerable\"\n",
        "      }\n",
        "\n",
        "    elif rf_prob >= threshold or rule_flag:\n",
        "      rf_result = {\n",
        "        \"hallucinated\": True,\n",
        "        \"confidence\": round(rf_prob, 3),\n",
        "        \"reason\": \"hallucination\"\n",
        "      }\n",
        "\n",
        "    elif rf_prob < MIN_CONFIDENCE:\n",
        "      rf_result = {\n",
        "        \"hallucinated\": True,\n",
        "        \"confidence\": round(rf_prob, 3),\n",
        "        \"reason\": \"low_confidence\"\n",
        "      }\n",
        "\n",
        "    elif rf_prob == 0.0:\n",
        "      rf_result = {\n",
        "        \"hallucinated\": True,\n",
        "        \"confidence\": 0.0,\n",
        "        \"reason\": \"unvalidated\"\n",
        "    }\n",
        "\n",
        "    else:\n",
        "      rf_result = {\n",
        "        \"hallucinated\": False,\n",
        "        \"confidence\": round(rf_prob, 3),\n",
        "        \"reason\": \"grounded\"\n",
        "      }\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 7ï¸âƒ£ MEMORY STORAGE (SAFE ONLY)\n",
        "    # ===============================\n",
        "    if (\n",
        "    not is_scenario_question(question)\n",
        "    and not is_cross_policy_question(question)\n",
        "    and answer.strip() != \"Information not found in the provided documents.\"\n",
        "    ):\n",
        "      store_memory(question, answer)\n",
        "\n",
        "\n",
        "\n",
        "    # ===============================\n",
        "    # 8ï¸âƒ£ FINAL RETURN\n",
        "    # ===============================\n",
        "    return answer, sources, rf_result\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# CHAT LOOP\n",
        "# ======================================================\n",
        "print(\"\\nâœ… RAG + MEMORY + FAISS + GEMINI + RF READY (COLAB)\\n\")\n",
        "\n",
        "while True:\n",
        "    q = input(\"â“ Ask a question: \")\n",
        "    if q.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    answer, sources, rf_result = rag_answer(q)\n",
        "\n",
        "    print(\"\\nðŸ§  Answer:\\n\", answer)\n",
        "\n",
        "    # ===============================\n",
        "    # HALLUCINATION STATUS\n",
        "    # ===============================\n",
        "    if rf_result is None:\n",
        "      print(\"\\nâš ï¸ No confidence evaluation available\")\n",
        "    else:\n",
        "      if rf_result[\"reason\"] == \"grounded\":\n",
        "          print(f\"\\nâœ… Answer is grounded (confidence={rf_result['confidence']})\")\n",
        "\n",
        "      elif rf_result[\"reason\"] == \"memory_reuse\":\n",
        "          print(\"\\nðŸ§  Answer reused from memory (cached)\")\n",
        "\n",
        "      elif rf_result[\"reason\"] == \"unanswerable\":\n",
        "          print(\"\\nâš ï¸ Answer not found in documents\")\n",
        "\n",
        "      elif rf_result[\"reason\"] == \"low_confidence\":\n",
        "          print(f\"\\nâš ï¸ Answer unreliable (confidence={rf_result['confidence']})\")\n",
        "\n",
        "      else:\n",
        "          print(f\"\\nâŒ Possible hallucination (confidence={rf_result['confidence']})\")\n",
        "\n",
        "    # ===============================\n",
        "    # SOURCES\n",
        "    # ===============================\n",
        "    if sources:\n",
        "        print(\"\\nðŸ“š Sources:\")\n",
        "        for s in sources:\n",
        "            print(\" -\", s)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "id": "j_ukWg52o4dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for f in [\n",
        "    DOC_INDEX_PATH,\n",
        "    TEXTS_PATH,\n",
        "    METADATA_PATH,\n",
        "    INDEXED_FILES_PATH\n",
        "]:\n",
        "    if os.path.exists(f):\n",
        "        os.remove(f)\n",
        "\n",
        "print(\"Old FAISS index deleted. Re-run notebook.\")"
      ],
      "metadata": {
        "id": "jKWxOLERpLHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}